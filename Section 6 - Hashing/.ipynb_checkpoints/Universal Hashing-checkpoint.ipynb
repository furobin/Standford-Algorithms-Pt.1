{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Load of a Hash Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The Load Factor (alpha) of a hash table is = # of objects in hash table/ # of buckets in hash table. \n",
    "     - Only chaining is feasible for load factors larger than 1. This is bc open addressing requires at most 1 obj per bucket, so up to alpha = 1. \n",
    " - 1. alpha = O(1) is necessary condition for operations ot run in constant time. EX: if have n buckets, hash in nlogn objects, you will have alpha - O(logn). So, lokoup will be basically O(logn).\n",
    " - 2. With open addressing, need alpha < 1 (pretty far below 1, alpha = 0.9 pretty bad). \n",
    " \n",
    "Upshots:\n",
    " - Can control # of buckets within hash table. Implementations keep track of # objects in table, ensures denominator #buckets grows at same rate for most part. \n",
    "     - EX: if alpha gets to 0.75, hash table doubles # of buckets and creates new table. Somethin like that.\n",
    " - For good hash table performance, also need good hash func that spread data evenly among buckets. \n",
    "     - Best if works independently of data. True for p much all algorithms we've been considering. \n",
    "     - Problem: this hash function does not exist. For every hash function, there is a pathological data set (kryptonite) that will make the hash function very poor. \n",
    "         - Fix any hash function h: U -> (0,1,2,...n-1), maps U to buckets, |U| >> n (cardinality of U much greater than n)\n",
    "         - Based on Pigeonhole, there exists bucket i such that |U|/n elements of U hash to i under h. \n",
    "         - Basically, at least one bucket has U/n population if considering entire universe. Bucket gets at least its \"fair share\" of data. Lets say, bucket is 31 can get its \"fair share\"\n",
    "             - Pathological data set, entire data set is all the items that would be mapped to 31. Screwed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathological Data in the Real World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: Crosby and Wallach,  USENIX 2003\n",
    "\n",
    "Main Point: Can paralyze several real-world systems (e.g. network intrusion detection) by exploiting badly designed hash functions using pathological data sets. These systems dependent on hash tables, can crash those tables. Bad functions:\n",
    " - Open Sourced\n",
    " - Overly simplistic hash function, easy to reverse engineer a pathological data set to break hash table to devolve performance to linear O(n). \n",
    " \n",
    "Solutions:\n",
    " - Cryptographic Hash Function (e.g. SHA-2). Still have pathological data sets but infeasible to reverse-engineer a pathological data set. \n",
    "     - More practical to counter people trying to construct pathological data sets\n",
    " - Use randomization\n",
    "     - Design a family H of hash functions such that, at runtime, pick one of hash functions at random. H designed such that, for all data sets S, \"almost all\" functions h in H spread S out \"pretty evenly\" \n",
    "     - Think of Quicksort where pivots chosen randomly at runtime\n",
    "     - Can still have code be open source since random. No clue abt what real-time random choice is by algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Randomization - Universal Hash Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal Hash Function - Let H be a set of hash functions from U to [0,1,2,3...n-1]. He is universal iff:\n",
    " - For all distinct keys from U, x and y, Pr[x and y collide with h(x) = h(y)] <= 1/n, n buckets, when h is chosen uniformly at random from H\n",
    " - i.e. collision probablility should be as small as \"gold standard\" of completely random hasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider hash fam H where each has function maps elements from U to n buckets. Suppose H has the property: for every bucket i and key k, a 1/n fraction of the has functions in H map k to i. Is H universal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes: Take H = all functions from that map U to {0,1,2,3...n-1}. Huge set. By symmetry, satisfies the conditions above. Basically, perfect random function like our gold standard. \n",
    "\n",
    "No: Take H = family of eactly n functions, each function is a constant function (i.e. one function maps to 0, one to 1, one to 2, etc.). Does fit conditions above. I dont get how not a universal tho tbh\n",
    "\n",
    "So, maybe yes, maybe no. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate usefulness of mathematical definition:\n",
    " - Show it is satisfied by objects of interest. i.e., show that hash functions we might want to implement are easy to store and evaluate and meet definition. \n",
    " - If meet the definition, something good comes about. \n",
    "\n",
    "Example: Hashing IP Addresses\n",
    "\n",
    "Let U = IP addresses (of the form (x1, x2, x, x4) with each xi in 0,1,2,3...255). IP address is 32bit integer with 4 different 8-bit parts. Since each of 4 parts is 8 bits, it can take 2^8 possible values so 0 - 255.\n",
    " - Let n = a prime (e.g. small multiple of # of objects in hash table), # of buckets. Lets say want to store 500 IP addresses, choose n = 997 or something like that. \n",
    " - Construction:\n",
    "     - Define one hash function ha per 4-tuple, a = (a1, a2, a3, a4; all integers) with each ai in (0,1,2,3...n-1), components of a. Defines n^4 functions since each ai has n options. \n",
    " - Define:\n",
    "     - ha: IP address -> buckets mapped\n",
    "     - By ha(x1, x2, x3, x4) = (a1x1 + a2x2 + a3x3 + a4x4) % n. Outputs number betweeen 0 and n-1 as desired. \n",
    "     - Storage per h: remembers a1,a2,a3,a4, pretty good. Constant space to store.\n",
    "     - Evaluation also very easy, constant time. Just basic operations.\n",
    "     - Also meets definition of universal hash function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define: H = (ha | a1,a2,a3,a4)\n",
    "\n",
    "Theorem: This family is universal.\n",
    "- Need to use assumption that n is prime, will involve some number theory lmao. \n",
    "\n",
    "Proof Part 1:\n",
    " - Consider distinct IP addresses (x1,x2,x3,x4), (y1,y2,y3,y4)\n",
    " - Assume: x4 != y4, makes keys distinct. Doesn't matter which one look at or even look at more than 1 set of 8 bits. \n",
    " - Question: Need to find an upper bound on the collision probability. That is, given 2 IP addresses, what fraction of the hash functions in H cause these 2 to collide? Need at most 1/n fraction sends them to the same bucket. \n",
    " - Note: Collision occurs when: a1x1 + a2x3 + a3x3 + a4x5 modn = a1y1 + a2y2 + a3y3 + a4y4 modn\n",
    "     - a4(x4 - y4) modn = a1(y1 - x1) + a2 (y2-x2) + a3 (y3-x3) modn\n",
    "     - Condition on random choices of a1, a2, a3; a4 still random. Uses Principles of Deferred Decisions. \n",
    "     - With a1,a2,a3 fixed, how many choices of a4 satisfy the equation above? Need at most 1/n fraction. \n",
    "         - By fixing a1-3, right hand side of EQN is now some fixed number in range(n); a4 still random.\n",
    "     - Key Claim: Left-Hand side equally likely to be any number in range(n). \n",
    "         - To be thorough, need number theory. This below is a little hand-wavy\n",
    "         - x4 != y4, so x4 - y4 != 0. (Addendum: make sure n bigger than maximum value of a coefficient ai)\n",
    "         - n is prime, a4 is random in range(n). \n",
    "         - \"Proof\" by example:\n",
    "             - Let's say there are 7 buckets, x4 - y4 = 2. Look at each possible choice for a4 and how it affects left-hand side (LHS)\n",
    "             - a4 = 0, LHS = 0. a4 = 1, LHS = 2. a3 = 2, LHS = 4. So on. Gets 0, 2, 4, 6, 1, 3, 5 with all a4s. So, if a4 chosen random, LHS will be random in range(n). \n",
    "     - Thus, since LHS is uniformly random in range(n) and RHS = some number in range(n), chance that LHS = RHS is 1/n. So, H is a universal family. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview Again:\n",
    " - Proposed definition of a \"good random hash function\"\n",
    " - Provided example of simple and practical functions above\n",
    " - Justification and definition: \"good functions\" lead to \"good performance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining: Constant-Time Guarantee\n",
    "\n",
    "Scenario: hash table implemented with chaining. Hash function h hosen uniformly at random from universal family H\n",
    "\n",
    "Theorem: All operations run in O(1) time [Carter-Wegman 1979]\n",
    " - No assumptions about the data set, doesn't matter what is storing in hash. \n",
    "\n",
    "Caveats:\n",
    " - In expectation over random choice of the hash function h\n",
    " - Assumes |S| = O(n), that is, load alpha = |S|/n = O(1); n = # of buckets. \n",
    " - assume O(1) time to evaluate hash function h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    " - Bound running time of an unsuccessful lookup, good enough to bound the running time of all these operations. Remember, first hash appropriate bucket, then look for item in list at that bucket. \n",
    " - Let S = data set with |S| = O(n). Consider lookup for x not in S (arbitrary data set). \n",
    "\n",
    "Running Time: O(1) [eval hash function] + O(list length in A[h(x)]) [traversing list to find x], call list length L. \n",
    " - L is a random var, function of some function h in H. So, must analyze L.\n",
    "\n",
    "General Decomposition Principle:\n",
    " - Identify random variable Y that we care about\n",
    " - Express Y as sum of indicator random variables Xi (that are 0 or 1)\n",
    " - Apply linearity of expectation E(Y) = sum(E(Xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDP Applied:\n",
    " - Let L = length in A[h(x)]\n",
    " - For y in S (remember x not in S), define each random variable Zy = 1 if h(y) = h(x), 0 otherwise\n",
    "     - Note, L = sum(Zy) for all y in S. \n",
    " - So, E[L] = sum(E[Zy]) via linearity of expectation; E[Zy] = Pr(h(y) = h(x))\n",
    "     - Based on definition of universal hashing function, Pr(h(y) = h(x)) <= 1/n. \n",
    "     - So, E[L] = sum(1/n); since |S| = O(n), E[L] = O(1). \n",
    "     - Thus, operations O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Open Addressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: only one object stored per bucket in hash table. alpha << 1 (# obj / # buckets). Hash function produces a probe sequence for each possible key x.\n",
    " - Difficult to analyze rigorously\n",
    " \n",
    "Heuristic Assumption (for quick and dirty idealized analysis only): all n! probe sequences are equally likely. \n",
    " - Then, expected Insertion time is 1/(1-alpha); lookup is at least as good as insert.\n",
    " - Remember problem, if alpha even near 1, very bad. Keep well below 0.7. \n",
    " \n",
    "Proof: A random probe finds an empty slot with probability 1 - alpha. alpha is basically % of slots filled. \n",
    " - So, insertion time ~ the number N of coin flips to get a successful slot 1 - alpha. \n",
    "     - This \"coin flip\" actually slightly over-estimates insertion time under heuristic bc never try same slot twice. \n",
    " - E[N] = 1 (necessary flip) + alpha * E[N] (prob of tails, if tails then  restarts so mult by further flips needed)\n",
    " - E[N] = 1/(1-alpha)\n",
    " - May not hold in practice, but can use this as a reference for implementation\n",
    " \n",
    "Double Hashing -  try to look for this bound. Can try to reach for heuristic basically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Probing - will not see this performance guarantee even if in completely ideal situation.\n",
    " - The heuristic assumption is completely false; will be subject to clumping. \n",
    " - Still, will have reasonable performance. \n",
    " - Assume Instead - initial probe uniformly random and independent for different keys. Very strong assumption but performance guarantees derived with this assumption pretty practial\n",
    " \n",
    "Theorem: [Knuth 1962] under above assumption, expected Insertion time is about 1/(1-alpha)^2; good bc not dependent on number of objects in hash table but still on load. \n",
    " - Really load to be very low. \n",
    " \n",
    "Why use linear probing? Depends on application but pretty common in practice. Tends to interact well with memory hierarchies. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
